# Анализ именованных сущностей (NER) с помощью GigaChat

Проект выполняет анализ качества извлечения именованных сущностей из новостных текстов на тему Brexit, используя датасет BSNLP-2019. Исследуются пять типов сущностей: персоны (PER), организации (ORG), локации (LOC), события (EVT) и продукты (PRO).

## Структура проекта

```
├── NERbyGigaChat.ipynb          # Основной ноутбук с анализом
├── score_fn.py                  # Модуль для вычисления метрик
├── tests/                       # Юнит-тесты
│   └── test_score_fn.py        
├── gigachat_result.json         # Результаты запросов GigaChat
├── ner_brexit_ru_predictions.csv # Обработанные результаты в csv
├── .gitignore                   # Игнорируемые файлы
└── README.md                    # Этот файл
```

## Запуск тестов
```bash
pytest tests/ -v
```

## Основные результаты

### Метрики качества

| Метрика    | По сущностям | По документам |
|------------|:------------:|:-------------:|
| F1         | 0.284        | 0.364         |
| Precision  | 0.566        | 0.529         |
| Recall     | 0.197        | 0.284         |

### Анализ по типам сущностей

| Тип сущности | Precision | Recall | F1     |
|--------------|:---------:|:------:|:------:|
| EVT          | 0.667     | 0.118  | 0.200  |
| LOC          | 0.450     | 0.191  | 0.269  |
| ORG          | 0.463     | 0.244  | 0.319  |
| PER          | 0.583     | 0.266  | 0.365  |
| PRO          | 0.667     | 0.167  | 0.267  |


### Зависимость от длины документа
Выявлена **статистически значимая отрицательная корреляция** между длиной документа и качеством извлечения:
- **Recall**: r = -0.584 (p = 0.017)
- **F1-score**: r = -0.559 (p = 0.024)